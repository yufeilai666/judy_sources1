import os
import sys
import re
import json
import time
import random
import argparse
import requests
import datetime
import pytz
from bs4 import BeautifulSoup
from xml.etree import ElementTree as ET
from xml.dom import minidom

# å…¨å±€æ—¶åŒºè®¾ç½®
TAIPEI_TZ = pytz.timezone('Asia/Taipei')

# ä»£ç†è®¾ç½®
HTTP_PROXY = os.environ.get('http_proxy', '') or os.environ.get('HTTP_PROXY', '')
HTTPS_PROXY = os.environ.get('https_proxy', '') or os.environ.get('HTTPS_PROXY', '')

PROXIES = {}
if HTTP_PROXY:
    PROXIES['http'] = HTTP_PROXY
if HTTPS_PROXY:
    PROXIES['https'] = HTTPS_PROXY

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def create_session():
    """åˆ›å»ºå¸¦æœ‰ä»£ç†çš„ä¼šè¯"""
    session = requests.Session()
    session.headers.update(HEADERS)
    
    if PROXIES:
        print(f"ä½¿ç”¨ä»£ç†: {PROXIES}")
        session.proxies.update(PROXIES)
    else:
        print("æœªè®¾ç½®ä»£ç†ï¼Œä½¿ç”¨ç›´æ¥è¿æ¥")
    
    return session

def parse_channel_list(session):
    """ä»LiTV APIè·å–é¢‘é“æ¸…å•ï¼ŒåªæŠ“å–ç‰¹å®šIDæ¨¡å¼çš„é¢‘é“"""
    print("å¼€å§‹è·å–LiTVé¢‘é“æ¸…å•...")
    
    # LiTVé¢‘é“API
    channel_url = "https://www.litv.tv/_next/data/322e31352e3138/channel.json"
    
    try:
        response = session.get(channel_url, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        # ä» pageProps.introduction.channels è·å–é¢‘é“åˆ—è¡¨
        channels_data = data.get('pageProps', {}).get('introduction', {}).get('channels', [])
        
        if not channels_data:
            print("âŒ æœªæ‰¾åˆ°é¢‘é“æ•°æ®")
            return []
        
        print(f"æ‰¾åˆ° {len(channels_data)} ä¸ªé¢‘é“")
        
        # å®šä¹‰è¦æŠ“å–çš„é¢‘é“IDæ¨¡å¼
        target_patterns = [
            r'^4gtv-4gtv.*',      # 4gtv-4gtvå¼€å¤´çš„æ‰€æœ‰é¢‘é“
            r'^litv-ftv.*',       # litv-ftvå¼€å¤´çš„æ‰€æœ‰é¢‘é“
            r'^iNEWS$',           # ç²¾ç¡®åŒ¹é…iNEWS
            r'^litv-longturn.*'   # litv-longturnå¼€å¤´çš„æ‰€æœ‰é¢‘é“
        ]
        
        channels = []
        for channel in channels_data:
            channel_name = channel.get('title', '').strip()
            channel_id = channel.get('cdn_code', '').strip()
            
            if not channel_name or not channel_id:
                continue
            
            # æ£€æŸ¥é¢‘é“IDæ˜¯å¦ç¬¦åˆç›®æ ‡æ¨¡å¼
            is_target = False
            for pattern in target_patterns:
                if re.match(pattern, channel_id):
                    is_target = True
                    break
            
            if not is_target:
                continue
                
            # å¤„ç†logo URL
            logo = channel.get('picture', '')
            if logo and not logo.startswith('http'):
                logo = f"https://fino.svc.litv.tv/{logo.lstrip('/')}"
            
            channels.append({
                "channelName": channel_name,
                "id": channel_id,
                "logo": logo,
                "description": channel.get('description', ''),
                "content_type": channel.get('content_type', 'channel')
            })
        
        print(f"âœ… æˆåŠŸè·å– {len(channels)} ä¸ªç›®æ ‡é¢‘é“")
        for channel in channels:
            print(f"   - {channel['channelName']} (ID: {channel['id']})")
        return channels
        
    except Exception as e:
        print(f"âŒ è·å–é¢‘é“æ¸…å•å¤±è´¥: {str(e)}")
        return []

def fetch_channel_epg(session, channel_id, channel_name):
    """ä»é¢‘é“é¡µé¢è·å–èŠ‚ç›®è¡¨æ•°æ® - æ–°æ–¹æ³•"""
    print(f"\nå¼€å§‹è·å–é¢‘é“ {channel_name} çš„èŠ‚ç›®è¡¨...")
    
    # é¢‘é“é¡µé¢URL
    channel_url = f"https://www.litv.tv/channel/watch/{channel_id}"
    
    try:
        response = session.get(channel_url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ä¿å­˜HTMLç”¨äºè°ƒè¯•
        with open(f"debug_{channel_id}.html", "w", encoding="utf-8") as f:
            f.write(soup.prettify())
        print(f"âœ… å·²ä¿å­˜HTMLåˆ° debug_{channel_id}.html ç”¨äºè°ƒè¯•")
        
        programs = []
        
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«èŠ‚ç›®è¡¨çš„å®¹å™¨
        # æ ¹æ®æ‚¨æä¾›çš„å›¾ç‰‡ï¼ŒèŠ‚ç›®è¡¨å¯èƒ½åœ¨ä¸€ä¸ªç‰¹å®šçš„å®¹å™¨ä¸­
        epg_containers = soup.find_all('div', class_=lambda x: x and 'overflow-y-auto' in x)
        
        if not epg_containers:
            print("âŒ æœªæ‰¾åˆ°èŠ‚ç›®è¡¨å®¹å™¨")
            return []
        
        print(f"æ‰¾åˆ° {len(epg_containers)} ä¸ªå¯èƒ½çš„èŠ‚ç›®è¡¨å®¹å™¨")
        
        for container in epg_containers:
            # æ£€æŸ¥å®¹å™¨æ˜¯å¦åŒ…å«èŠ‚ç›®ä¿¡æ¯
            container_text = container.get_text(strip=True)
            if re.search(r'\d{1,2}:\d{2}\s+.+', container_text):
                print("âœ… æ‰¾åˆ°åŒ…å«èŠ‚ç›®ä¿¡æ¯çš„å®¹å™¨")
                
                # æŸ¥æ‰¾æ‰€æœ‰æ—¥æœŸæ ‡é¢˜
                date_headers = container.find_all('div', class_=lambda x: x and 'text-[#fff]' in x)
                print(f"æ‰¾åˆ° {len(date_headers)} ä¸ªæ—¥æœŸæ ‡é¢˜")
                
                for date_header in date_headers:
                    date_text = date_header.get_text(strip=True)
                    print(f"å¤„ç†æ—¥æœŸ: {date_text}")
                    
                    # è§£ææ—¥æœŸ
                    current_date = parse_date_from_text(date_text)
                    if not current_date:
                        continue
                    
                    # æŸ¥æ‰¾è¿™ä¸ªæ—¥æœŸä¸‹çš„æ‰€æœ‰èŠ‚ç›®
                    # æŸ¥æ‰¾æ—¥æœŸæ ‡é¢˜åé¢çš„æ‰€æœ‰èŠ‚ç›®è¡Œ
                    next_elem = date_header.find_next_sibling()
                    while next_elem:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯èŠ‚ç›®è¡Œ
                        if next_elem.name == 'div' and next_elem.get('class'):
                            class_str = ' '.join(next_elem.get('class', []))
                            program_text = next_elem.get_text(strip=True)
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ—¥æœŸæ ‡é¢˜
                            if re.search(r'\d+æœˆ\d+æ—¥', program_text):
                                break
                                
                            # æ£€æŸ¥æ˜¯å¦æ˜¯èŠ‚ç›®è¡Œ
                            time_match = re.match(r'(\d{1,2}):(\d{2})\s+(.+)', program_text)
                            if time_match:
                                hour = int(time_match.group(1))
                                minute = int(time_match.group(2))
                                program_name = time_match.group(3)
                                
                                # è®¡ç®—èŠ‚ç›®å¼€å§‹æ—¶é—´
                                program_start = current_date.replace(hour=hour, minute=minute, second=0)
                                
                                # é¢„è®¾èŠ‚ç›®æ—¶é•¿ä¸º1å°æ—¶
                                program_end = program_start + datetime.timedelta(hours=1)
                                
                                programs.append({
                                    "channelName": channel_name,
                                    "programName": program_name,
                                    "description": "",
                                    "subtitle": "",
                                    "start": program_start,
                                    "end": program_end
                                })
                                
                                print(f"  èŠ‚ç›®: {hour:02d}:{minute:02d} - {program_name}")
                        
                        next_elem = next_elem.find_next_sibling()
                
                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„èŠ‚ç›®è¡¨å®¹å™¨
        
        # æ–¹æ³•2: å¦‚æœä¸Šé¢æ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥æœç´¢æ‰€æœ‰åŒ…å«æ—¶é—´å’ŒèŠ‚ç›®åç§°çš„å…ƒç´ 
        if not programs:
            print("å°è¯•æ–¹æ³•2: ç›´æ¥æœç´¢æ‰€æœ‰èŠ‚ç›®è¡Œ")
            all_elements = soup.find_all(text=re.compile(r'\d{1,2}:\d{2}\s+.+'))
            for element in all_elements:
                text = element.strip()
                time_match = re.match(r'(\d{1,2}):(\d{2})\s+(.+)', text)
                if time_match:
                    # ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºé»˜è®¤å€¼
                    current_date = datetime.datetime.now(TAIPEI_TZ).replace(hour=0, minute=0, second=0)
                    
                    hour = int(time_match.group(1))
                    minute = int(time_match.group(2))
                    program_name = time_match.group(3)
                    
                    program_start = current_date.replace(hour=hour, minute=minute, second=0)
                    program_end = program_start + datetime.timedelta(hours=1)
                    
                    programs.append({
                        "channelName": channel_name,
                        "programName": program_name,
                        "description": "",
                        "subtitle": "",
                        "start": program_start,
                        "end": program_end
                    })
                    
                    print(f"  èŠ‚ç›®: {hour:02d}:{minute:02d} - {program_name}")
        
        print(f"âœ… é¢‘é“ {channel_name} è·å–åˆ° {len(programs)} ä¸ªèŠ‚ç›®")
        return programs
        
    except Exception as e:
        print(f"âŒ è·å–é¢‘é“ {channel_name} èŠ‚ç›®è¡¨å¤±è´¥: {str(e)}")
        return []

def parse_date_from_text(date_text):
    """ä»æ—¥æœŸæ–‡æœ¬è§£ææ—¥æœŸ"""
    try:
        # å¤„ç† "ä»Šæ—¥ / 11æœˆ1æ—¥ / æ˜ŸæœŸå…­" æˆ– "æ˜æ—¥ / 11æœˆ2æ—¥ / æ˜ŸæœŸäº”" æ ¼å¼
        parts = date_text.split(' / ')
        if len(parts) >= 2:
            date_part = parts[1]  # "11æœˆ1æ—¥"
            
            # è·å–å½“å‰å¹´ä»½
            current_year = datetime.datetime.now().year
            
            # è§£ææœˆä»½å’Œæ—¥æœŸ
            month_match = re.search(r'(\d+)æœˆ', date_part)
            day_match = re.search(r'(\d+)æ—¥', date_part)
            
            if month_match and day_match:
                month = int(month_match.group(1))
                day = int(day_match.group(1))
                
                # åˆ›å»ºæ—¥æœŸå¯¹è±¡
                date_obj = datetime.datetime(current_year, month, day, tzinfo=TAIPEI_TZ)
                return date_obj
    except Exception as e:
        print(f"âš ï¸ æ—¥æœŸè§£æå¤±è´¥: {date_text}, é”™è¯¯: {str(e)}")
    
    return None

def get_litv_epg():
    """è·å–LiTVç”µè§†èŠ‚ç›®è¡¨"""
    print("="*50)
    print("å¼€å§‹è·å–LiTVç”µè§†èŠ‚ç›®è¡¨")
    print("="*50)
    
    # åˆ›å»ºä¼šè¯
    session = create_session()
    
    # è·å–é¢‘é“æ¸…å•
    channels_info = parse_channel_list(session)
    if not channels_info:
        print("âŒ æ— æ³•è·å–é¢‘é“æ¸…å•")
        return [], [], []  # è¿”å›ä¸‰ä¸ªç©ºåˆ—è¡¨
    
    # ä¸ºæ¯ä¸ªé¢‘é“è·å–èŠ‚ç›®è¡¨
    all_programs = []
    for channel in channels_info:
        channel_id = channel["id"]
        channel_name = channel["channelName"]
        
        # è·å–è¯¥é¢‘é“çš„èŠ‚ç›®è¡¨
        programs = fetch_channel_epg(session, channel_id, channel_name)
        all_programs.extend(programs)
        
        # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        delay = random.uniform(2, 5)
        print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
        time.sleep(delay)
    
    # æ ¼å¼åŒ–é¢‘é“èµ„è®¯ï¼ˆç”¨äºXMLTVç”Ÿæˆï¼‰
    all_channels = []
    for channel in channels_info:
        channel_info = {
            "name": channel["channelName"],
            "channelName": channel["channelName"],
            "id": channel["id"],
            "url": f"https://www.litv.tv/channel/{channel['id']}",
            "source": "litv",
            "desc": channel.get("description", ""),
            "sort": "å°æ¹¾"
        }
        
        if channel.get("logo"):
            channel_info["logo"] = channel["logo"]
        
        all_channels.append(channel_info)
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*50)
    print(f"âœ… æˆåŠŸè·å– {len(all_channels)} ä¸ªé¢‘é“")
    print(f"âœ… æˆåŠŸè·å– {len(all_programs)} ä¸ªèŠ‚ç›®")
    
    # æŒ‰é¢‘é“åç§°åˆ†ç»„æ˜¾ç¤ºèŠ‚ç›®æ•°é‡
    channel_counts = {}
    for program in all_programs:
        channel_counts[program["channelName"]] = channel_counts.get(program["channelName"], 0) + 1
    
    for channel, count in channel_counts.items():
        print(f"ğŸ“º é¢‘é“ {channel}: {count} ä¸ªèŠ‚ç›®")
    
    print("="*50)
    return channels_info, all_channels, all_programs

def generate_xmltv(channels, programs, output_file="litv.xml"):
    """ç”ŸæˆXMLTVæ ¼å¼çš„EPGæ•°æ®"""
    print(f"\nç”ŸæˆXMLTVæ¡£æ¡ˆ: {output_file}")
    
    if not channels or not programs:
        print("âŒ æ²¡æœ‰é¢‘é“æˆ–èŠ‚ç›®æ•°æ®ï¼Œæ— æ³•ç”ŸæˆXMLTV")
        return False
    
    # å»ºç«‹XMLæ ¹å…ƒç´ 
    root = ET.Element("tv", generator="LITV-EPG-Generator", source="www.litv.tv")
    
    program_count = 0
    for channel in channels:
        channel_name = channel['name']
        
        # æ·»åŠ é¢‘é“å®šä¹‰
        channel_elem = ET.SubElement(root, "channel", id=channel_name)
        ET.SubElement(channel_elem, "display-name", lang="zh").text = channel_name
        
        if channel.get('logo'):
            ET.SubElement(channel_elem, "icon", src=channel['logo'])
        
        # è·å–è¯¥é¢‘é“çš„æ‰€æœ‰èŠ‚ç›®
        channel_programs = [p for p in programs if p['channelName'] == channel_name]
        if not channel_programs:
            print(f"âš ï¸ é¢‘é“ {channel_name} æ²¡æœ‰èŠ‚ç›®æ•°æ®")
            continue
            
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        channel_programs.sort(key=lambda p: p['start'])
        
        # æ·»åŠ è¯¥é¢‘é“çš„æ‰€æœ‰èŠ‚ç›®
        for program in channel_programs:
            try:
                start_time = program['start'].strftime('%Y%m%d%H%M%S %z')
                end_time = program['end'].strftime('%Y%m%d%H%M%S %z')
                
                program_elem = ET.SubElement(
                    root, 
                    "programme", 
                    channel=channel_name,
                    start=start_time, 
                    stop=end_time
                )
                
                title = program.get('programName', 'æœªçŸ¥èŠ‚ç›®')
                ET.SubElement(program_elem, "title", lang="zh").text = title
                
                program_count += 1
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„èŠ‚ç›®æ•°æ®: {str(e)}")
                continue
    
    # ç”ŸæˆXMLå­—ç¬¦ä¸²
    xml_str = ET.tostring(root, encoding='utf-8').decode('utf-8')
    
    # ç¾åŒ–XMLæ ¼å¼
    try:
        parsed = minidom.parseString(xml_str)
        pretty_xml = parsed.toprettyxml(indent="  ", encoding='utf-8')
    except Exception as e:
        print(f"âš ï¸ XMLç¾åŒ–å¤±è´¥, ä½¿ç”¨åŸå§‹XML: {str(e)}")
        pretty_xml = xml_str.encode('utf-8')
    
    # å‚¨å­˜åˆ°æ¡£æ¡ˆ
    try:
        with open(output_file, 'wb') as f:
            f.write(pretty_xml)
        
        print(f"âœ… XMLTVæ¡£æ¡ˆå·²ç”Ÿæˆ: {output_file}")
        print(f"ğŸ“º é¢‘é“æ•°: {len(channels)}")
        print(f"ğŸ“º èŠ‚ç›®æ•°: {program_count}")
        return True
    except Exception as e:
        print(f"âŒ å‚¨å­˜XMLæ¡£æ¡ˆå¤±è´¥: {str(e)}")
        return False

def generate_channel_json(channels_info, output_file="litv.json"):
    """ç”ŸæˆJSONæ ¼å¼çš„é¢‘é“èµ„è®¯"""
    print(f"\nç”ŸæˆJSONé¢‘é“æ¡£æ¡ˆ: {output_file}")
    
    if not channels_info:
        print("âŒ æ²¡æœ‰é¢‘é“æ•°æ®ï¼Œæ— æ³•ç”ŸæˆJSON")
        return False
    
    try:
        # æ ¼å¼åŒ–é¢‘é“èµ„è®¯ä¸ºæ‰€éœ€çš„JSONæ ¼å¼
        json_channels = []
        for channel in channels_info:
            json_channel = {
                "channelName": channel["channelName"],
                "id": channel["id"],
                "logo": channel.get("logo", ""),
                "description": channel.get("description", "")
            }
            json_channels.append(json_channel)
        
        # å†™å…¥JSONæ¡£æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_channels, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONé¢‘é“æ¡£æ¡ˆå·²ç”Ÿæˆ: {output_file}")
        print(f"ğŸ“º é¢‘é“æ•°: {len(json_channels)}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆJSONé¢‘é“æ¡£æ¡ˆå¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='LiTVç”µè§†èŠ‚ç›®è¡¨')
    parser.add_argument('--output', type=str, default='output/litv.xml', 
                       help='è¾“å‡ºXMLæ¡£æ¡ˆè·¯å¾„ (é»˜è®¤: output/litv.xml)')
    parser.add_argument('--json', type=str, default='output/litv.json',
                       help='è¾“å‡ºJSONé¢‘é“æ¡£æ¡ˆè·¯å¾„ (é»˜è®¤: output/litv.json)')
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"å»ºç«‹è¾“å‡ºç›®å½•: {output_dir}")
    
    json_dir = os.path.dirname(args.json)
    if json_dir and not os.path.exists(json_dir):
        os.makedirs(json_dir, exist_ok=True)
        print(f"å»ºç«‹JSONè¾“å‡ºç›®å½•: {json_dir}")
    
    try:
        # è·å–EPGæ•°æ®
        channels_info, all_channels, programs = get_litv_epg()
        
        if not channels_info:
            print("âŒ æœªè·å–åˆ°é¢‘é“æ•°æ®ï¼Œæ— æ³•ç”ŸæˆXMLå’ŒJSON")
            sys.exit(1)
            
        # ç”ŸæˆXMLTVæ¡£æ¡ˆ
        if not generate_xmltv(all_channels, programs, args.output):
            print("âš ï¸ XMLTVæ¡£æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œä½†ç»§ç»­ç”ŸæˆJSONæ¡£æ¡ˆ")
            
        # ç”ŸæˆJSONé¢‘é“æ¡£æ¡ˆ
        if not generate_channel_json(channels_info, args.json):
            print("âŒ JSONé¢‘é“æ¡£æ¡ˆç”Ÿæˆå¤±è´¥")
            sys.exit(1)
            
        print(f"\nğŸ‰ æ‰€æœ‰æ¡£æ¡ˆç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“„ XMLTV EPGæ¡£æ¡ˆ: {args.output}")
        print(f"ğŸ“„ JSONé¢‘é“æ¡£æ¡ˆ: {args.json}")
            
    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºé”™è¯¯: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
