import os
import sys
import re
import json
import time
import random
import argparse
import requests
import datetime
import pytz
from bs4 import BeautifulSoup
from xml.etree import ElementTree as ET
from xml.dom import minidom

# å…¨å±€æ™‚å€è¨­ç½®
TAIPEI_TZ = pytz.timezone('Asia/Taipei')
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def parse_channel_list():
    """å¾ç¶²é å‹•æ…‹è§£æé »é“æ¸…å–®"""
    # å˜—è©¦ä½¿ç”¨é »é“åˆ—è¡¨é é¢
    url = "https://www.ofiii.com/channel"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ–¹æ³•1: å¾__NEXT_DATA__ä¸­è§£æï¼ˆå¢å¼·ç‰ˆï¼‰
        script_tag = soup.find('script', id='__NEXT_DATA__')
        if script_tag and script_tag.string:
            try:
                data = json.loads(script_tag.string)
                print("ğŸ” åˆ†æ__NEXT_DATA__çµæ§‹...")
                
                # å¢å¼·çš„è§£ææ–¹æ³•
                channels_from_next_data = extract_channels_from_next_data_enhanced(data)
                if channels_from_next_data:
                    print(f"âœ… å¾__NEXT_DATA__è§£æåˆ° {len(channels_from_next_data)} å€‹é »é“")
                    return channels_from_next_data
                else:
                    print("âš ï¸ __NEXT_DATA__ä¸­æœªæ‰¾åˆ°é »é“åˆ—è¡¨ï¼Œå˜—è©¦èª¿è©¦...")
                    debug_next_data(data)  # èª¿è©¦å‡½æ•¸ï¼Œå¹«åŠ©åˆ†ææ•¸æ“šçµæ§‹
                    
            except json.JSONDecodeError as e:
                print(f"âš ï¸ __NEXT_DATA__ JSONè§£æå¤±æ•—: {str(e)}")
        
        # æ–¹æ³•2: å¾HTMLä¸­è§£ææ‰€æœ‰é »é“éˆæ¥
        print("ğŸ” å¾HTMLéˆæ¥è§£æé »é“...")
        channel_links = soup.find_all('a', href=re.compile(r'/channel/watch/'))
        if not channel_links:
            print("âŒ æœªæ‰¾åˆ°é »é“éˆæ¥")
            return []
        
        channel_list = []
        for link in channel_links:
            try:
                href = link.get('href', '')
                # æå–é »é“IDï¼ˆ/channel/watch/å¾Œé¢çš„éƒ¨åˆ†ï¼‰
                if '/channel/watch/' in href:
                    channel_id = href.split('/channel/watch/')[-1].strip('/')
                    if channel_id and channel_id not in channel_list:
                        channel_list.append(channel_id)
            except Exception as e:
                print(f"âš ï¸ è§£æé »é“éˆæ¥å¤±æ•—: {str(e)}")
                continue
        
        print(f"âœ… å¾HTMLéˆæ¥è§£æåˆ° {len(channel_list)} å€‹é »é“")
        
        # å¦‚æœHTMLè§£æçš„æ•¸é‡è¼ƒå°‘ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•è£œå……
        if len(channel_list) < 50:  # å‡è¨­å¯¦éš›é »é“æ•¸æ‡‰è©²å¤§æ–¼50
            print("âš ï¸ é »é“æ•¸é‡è¼ƒå°‘ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•è£œå……...")
            additional_channels = get_additional_channels(url, soup)
            for channel in additional_channels:
                if channel not in channel_list:
                    channel_list.append(channel)
            
            print(f"âœ… è£œå……å¾Œå…±æœ‰ {len(channel_list)} å€‹é »é“")
        
        return channel_list
        
    except Exception as e:
        print(f"âŒ å‹•æ…‹ç²å–é »é“åˆ—è¡¨å¤±æ•—: {str(e)}")
        return []

def extract_channels_from_next_data_enhanced(data):
    """å¢å¼·ç‰ˆï¼šå¾__NEXT_DATA__ä¸­æå–é »é“åˆ—è¡¨"""
    channels = []
    
    try:
        # æ–¹æ³•1: æ¨™æº–Next.jsçµæ§‹
        props = data.get('props', {})
        page_props = props.get('pageProps', {})
        
        # å˜—è©¦ä¸åŒçš„å¯èƒ½å­—æ®µåå’ŒåµŒå¥—çµæ§‹
        possible_paths = [
            ['props', 'pageProps', 'channels'],
            ['props', 'pageProps', 'channelList'],
            ['props', 'pageProps', 'items'],
            ['props', 'pageProps', 'data'],
            ['props', 'pageProps', 'initialState', 'channels'],
            ['props', 'pageProps', 'dehydratedState', 'queries'],
            ['props', 'pageProps', '__APOLLO_STATE__'],
            ['buildId'],
            ['page'],
            ['query'],
        ]
        
        for path in possible_paths:
            result = get_nested_value(data, path)
            if result:
                extracted = extract_channels_from_object(result)
                channels.extend(extracted)
        
        # æ–¹æ³•2: æœç´¢æ•´å€‹æ•¸æ“šçµæ§‹ä¸­çš„é »é“æ¨¡å¼
        if not channels:
            channels = search_channels_in_data_enhanced(data)
        
        # å»é‡
        channels = list(set(channels))
        
    except Exception as e:
        print(f"âš ï¸ å¾__NEXT_DATA__æå–é »é“å¤±æ•—: {str(e)}")
    
    return channels

def get_nested_value(obj, keys):
    """å®‰å…¨åœ°ç²å–åµŒå¥—å­—å…¸çš„å€¼"""
    try:
        for key in keys:
            if isinstance(obj, dict) and key in obj:
                obj = obj[key]
            else:
                return None
        return obj
    except:
        return None

def extract_channels_from_object(obj):
    """å¾å°è±¡ä¸­æå–é »é“ID"""
    channels = []
    
    if isinstance(obj, list):
        for item in obj:
            channels.extend(extract_channels_from_object(item))
    elif isinstance(obj, dict):
        # æª¢æŸ¥å¸¸è¦‹é »é“IDå­—æ®µ
        for key in ['id', 'channelId', 'slug', 'code', 'name', 'key']:
            if key in obj and isinstance(obj[key], str):
                channel_id = obj[key]
                if is_valid_channel_id(channel_id):
                    channels.append(channel_id)
        
        # éæ­¸æª¢æŸ¥æ‰€æœ‰å€¼
        for value in obj.values():
            channels.extend(extract_channels_from_object(value))
    
    return channels

def is_valid_channel_id(channel_id):
    """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„é »é“ID"""
    if not isinstance(channel_id, str):
        return False
    
    # æœ‰æ•ˆçš„é »é“IDæ¨¡å¼
    patterns = [
        r'^4gtv-',
        r'^litv-',
        r'^ofiii',
        r'^nnews-',
        r'^iNEWS',
        r'^daystar',
    ]
    
    for pattern in patterns:
        if re.search(pattern, channel_id):
            return True
    
    return False

def search_channels_in_data_enhanced(data, max_depth=5):
    """å¢å¼·ç‰ˆï¼šåœ¨æ•¸æ“šçµæ§‹ä¸­éæ­¸æœç´¢é »é“ID"""
    channels = []
    
    def _search(obj, depth=0, path=""):
        if depth > max_depth:
            return
        
        if isinstance(obj, dict):
            for key, value in obj.items():
                current_path = f"{path}.{key}" if path else key
                # å¦‚æœå€¼æ˜¯å­—ç¬¦ä¸²ï¼Œæª¢æŸ¥æ˜¯å¦æ˜¯é »é“ID
                if isinstance(value, str) and is_valid_channel_id(value):
                    if value not in channels:
                        channels.append(value)
                        print(f"ğŸ” åœ¨è·¯å¾‘ {current_path} æ‰¾åˆ°é »é“: {value}")
                else:
                    _search(value, depth + 1, current_path)
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                current_path = f"{path}[{i}]" if path else f"[{i}]"
                _search(item, depth + 1, current_path)
    
    _search(data)
    return channels

def debug_next_data(data):
    """èª¿è©¦å‡½æ•¸ï¼šåˆ†æ__NEXT_DATA__çµæ§‹"""
    print("ğŸ” èª¿è©¦__NEXT_DATA__çµæ§‹:")
    
    # æ‰“å°é ‚å±¤éµ
    print("é ‚å±¤éµ:", list(data.keys()))
    
    # æª¢æŸ¥propsçµæ§‹
    props = data.get('props', {})
    if props:
        print("propséµ:", list(props.keys()))
        
        page_props = props.get('pageProps', {})
        if page_props:
            print("pagePropséµ:", list(page_props.keys()))
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"channel"çš„éµ
    def find_channel_keys(obj, path=""):
        channel_keys = []
        
        if isinstance(obj, dict):
            for key, value in obj.items():
                current_path = f"{path}.{key}" if path else key
                if "channel" in key.lower():
                    channel_keys.append(current_path)
                channel_keys.extend(find_channel_keys(value, current_path))
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                current_path = f"{path}[{i}]" if path else f"[{i}]"
                channel_keys.extend(find_channel_keys(item, current_path))
        
        return channel_keys
    
    channel_keys = find_channel_keys(data)
    if channel_keys:
        print("åŒ…å«'channel'çš„éµ:", channel_keys[:10])  # åªé¡¯ç¤ºå‰10å€‹
    
    # çµ±è¨ˆæ•¸æ“šçµæ§‹å¤§å°
    def count_items(obj):
        if isinstance(obj, dict):
            return 1 + sum(count_items(v) for v in obj.values())
        elif isinstance(obj, list):
            return 1 + sum(count_items(item) for item in obj)
        else:
            return 1
    
    print("æ•¸æ“šçµæ§‹å¤§å°:", count_items(data))

def get_additional_channels(url, soup):
    """ç²å–é¡å¤–çš„é »é“åˆ—è¡¨"""
    additional_channels = []
    
    # æ–¹æ³•1: æŸ¥æ‰¾å¯èƒ½çš„APIç«¯é»
    scripts = soup.find_all('script')
    for script in scripts:
        if script.string:
            # æŸ¥æ‰¾å¯èƒ½çš„API URL
            api_patterns = [
                r'https?://[^"\']+api[^"\']+channels[^"\']*',
                r'https?://[^"\']+channels[^"\']*',
                r'/api/[^"\']+channels[^"\']*',
            ]
            
            for pattern in api_patterns:
                matches = re.findall(pattern, script.string)
                for match in matches:
                    print(f"ğŸ” ç™¼ç¾å¯èƒ½çš„APIç«¯é»: {match}")
                    # é€™é‡Œå¯ä»¥æ·»åŠ èª¿ç”¨APIçš„ä»£ç¢¼
    
    # æ–¹æ³•2: æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„é »é“åˆ—è¡¨å®¹å™¨
    containers = soup.find_all(['div', 'section'], class_=re.compile(r'.*(list|grid|container|channel).*', re.I))
    for container in containers:
        links = container.find_all('a', href=re.compile(r'/channel/watch/'))
        for link in links:
            href = link.get('href', '')
            if '/channel/watch/' in href:
                channel_id = href.split('/channel/watch/')[-1].strip('/')
                if channel_id and channel_id not in additional_channels:
                    additional_channels.append(channel_id)
    
    return additional_channels

def fetch_epg_data(channel_id, max_retries=3):
    """ç²å–æŒ‡å®šé »é“çš„é›»è¦–ç¯€ç›®è¡¨æ•¸æ“š"""
    url = f"https://www.ofiii.com/channel/watch/{channel_id}"
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            
            if not response.text.strip():
                print(f"âš ï¸ éŸ¿æ‡‰å…§å®¹ç‚ºç©º: {channel_id}")
                return None
                
            soup = BeautifulSoup(response.text, 'html.parser')
            script_tag = soup.find('script', id='__NEXT_DATA__')
            
            if script_tag and script_tag.string:
                try:
                    return json.loads(script_tag.string)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æå¤±æ•—: {channel_id}, {str(e)}")
                    return None
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°__NEXT_DATA__æ¨™ç°½: {channel_id}")
                return None
                
        except requests.RequestException as e:
            wait_time = random.uniform(1, 3) * (attempt + 1)
            print(f"âš ï¸ è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}), ç­‰å¾… {wait_time:.2f}ç§’: {str(e)}")
            time.sleep(wait_time)
    
    print(f"âŒ ç„¡æ³•ç²å– é›»è¦–ç¯€ç›®è¡¨ æ•¸æ“š: {channel_id}")
    return None

def parse_live_epg_data(json_data, channel_id):
    """è§£æç›´æ’­é »é“çš„é›»è¦–ç¯€ç›®è¡¨ JSONæ•¸æ“š"""
    if not json_data:
        return []
    
    programs = []
    try:
        if not json_data.get('props') or not json_data['props'].get('pageProps') or not json_data['props']['pageProps'].get('channel'):
            print(f"âŒ JSONçµæ§‹ç„¡æ•ˆ: {channel_id}")
            return []
        
        schedule = json_data['props']['pageProps']['channel'].get('Schedule', [])
        
        for item in schedule:
            try:
                start_utc = datetime.datetime.strptime(
                    item['AirDateTime'], "%Y-%m-%dT%H:%M:%SZ"
                ).replace(tzinfo=pytz.utc)
                start_taipei = start_utc.astimezone(TAIPEI_TZ)
                
                duration = datetime.timedelta(seconds=item.get('Duration', 0))
                end_taipei = start_taipei + duration
                
                program_info = item.get('program', {})
                
                programs.append({
                    "channelName": channel_id,
                    "programName": program_info.get('Title', 'æœªçŸ¥ç¯€ç›®'),
                    "description": program_info.get('Description', ''),
                    "subtitle": program_info.get('SubTitle', ''),
                    "start": start_taipei,
                    "end": end_taipei
                })
                
            except (KeyError, ValueError, TypeError) as e:
                print(f"âš ï¸ è·³éç„¡æ•ˆçš„ç¯€ç›®æ•¸æ“š: {channel_id}, {str(e)}")
                continue
                
    except (KeyError, TypeError, ValueError) as e:
        print(f"âŒ è§£æç›´æ’­é›»è¦–ç¯€ç›®è¡¨æ•¸æ“šå¤±æ•—: {str(e)}")
    
    return programs

def parse_vod_epg_data(json_data, channel_id):
    """è§£æé»æ’­é »é“çš„é›»è¦–ç¯€ç›®è¡¨ JSONæ•¸æ“š"""
    if not json_data:
        return []
    
    programs = []
    try:
        if not json_data.get('props') or not json_data['props'].get('pageProps') or not json_data['props']['pageProps'].get('channel'):
            print(f"âŒ JSONçµæ§‹ç„¡æ•ˆ: {channel_id}")
            return []
        
        channel_data = json_data['props']['pageProps']['channel']
        vod_schedule = channel_data.get('vod_channel_schedule', {})
        
        if not vod_schedule:
            print(f"âš ï¸ é»æ’­é »é“ {channel_id} æ²’æœ‰ç¯€ç›®è¡¨æ•¸æ“š")
            return []
        
        vod_programs = vod_schedule.get('programs', [])
        
        for item in vod_programs:
            try:
                start_timestamp = item.get('p_start', 0)
                if start_timestamp == 0:
                    continue
                    
                start_taipei = datetime.datetime.fromtimestamp(start_timestamp / 1000, TAIPEI_TZ)
                
                duration_ms = item.get('length', 0)
                duration = datetime.timedelta(milliseconds=duration_ms)
                end_taipei = start_taipei + duration
                
                programs.append({
                    "channelName": channel_id,
                    "programName": item.get('title', 'æœªçŸ¥ç¯€ç›®'),
                    "description": item.get('vod_channel_description', ''),
                    "subtitle": item.get('subtitle', ''),
                    "start": start_taipei,
                    "end": end_taipei
                })
                
            except (KeyError, ValueError, TypeError) as e:
                print(f"âš ï¸ è·³éç„¡æ•ˆçš„æ™‚é–“æ ¼å¼: {channel_id}, {str(e)}")
                continue
            
    except (KeyError, TypeError, ValueError) as e:
        print(f"âŒ è§£æé»æ’­é›»è¦–ç¯€ç›®è¡¨æ•¸æ“šå¤±æ•—: {str(e)}")
    
    return programs

def parse_epg_data(json_data, channel_id):
    """è§£æé›»è¦–ç¯€ç›®è¡¨ JSONæ•¸æ“šï¼Œè‡ªå‹•åˆ¤æ–·ç›´æ’­æˆ–é»æ’­"""
    if not json_data:
        return []
    
    try:
        channel_data = json_data['props']['pageProps']['channel']
        content_type = channel_data.get('content_type', '')
        
        if content_type == 'vod-channel' or channel_data.get('vod_channel_schedule'):
            print(f"ğŸ“¹ æª¢æ¸¬åˆ°é»æ’­é »é“: {channel_id}")
            return parse_vod_epg_data(json_data, channel_id)
        else:
            print(f"ğŸ“º æª¢æ¸¬åˆ°ç›´æ’­é »é“: {channel_id}")
            return parse_live_epg_data(json_data, channel_id)
            
    except (KeyError, TypeError, ValueError) as e:
        print(f"âŒ åˆ¤æ–·é »é“é¡å‹å¤±æ•—: {str(e)}")
        return parse_live_epg_data(json_data, channel_id)

def get_channel_info(json_data, channel_id):
    """å¾JSONæ•¸æ“šä¸­æå–é »é“ä¿¡æ¯"""
    if not json_data:
        return None
    
    try:
        page_props = json_data.get('props', {}).get('pageProps', {})
        channel_data = page_props.get('channel', {})
        
        # ç²å–é »é“åç¨±
        channel_name = channel_data.get('title', channel_id)
        
        # ç²å–é »é“logo
        logo = channel_data.get('picture', '')
        if logo and not logo.startswith("http"):
            logo = f"https://p-cdnstatic.svc.litv.tv/{logo}"
            # å°‡logoè·¯å¾‘ä¸­çš„_tvæ›¿æ›ç‚º_mobileä»¥ç²å–ç§»å‹•ç‰ˆlogo
            if '_tv' in logo:
                logo = logo.replace('_tv', '_mobile')
        
        # ç²å–é »é“æè¿°
        description = channel_data.get('description', '')
        
        return {
            "channelName": channel_name,
            "id": channel_id,
            "logo": logo,
            "description": description
        }
    except Exception as e:
        print(f"âŒ æå–é »é“ä¿¡æ¯å¤±æ•—: {channel_id}, {str(e)}")
        return None

def get_ofiii_epg():
    """ç²å–æ­é£›é›»è¦–ç¯€ç›®è¡¨"""
    print("="*50)
    print("é–‹å§‹ç²å–æ­é£›é›»è¦–ç¯€ç›®è¡¨")
    print("="*50)
    
    # ç²å–é »é“æ¸…å–®
    channels = parse_channel_list()
    if not channels:
        print("âŒ ç„¡æ³•è§£æé »é“æ¸…å–®")
        return [], []
    
    all_channels_info = []
    all_programs = []
    failed_channels = []
    
    # éæ­·æ‰€æœ‰é »é“
    for idx, channel_id in enumerate(channels):
        print(f"\nè™•ç†é »é“ [{idx+1}/{len(channels)}]: {channel_id}")
        
        # ç²å–EPGæ•¸æ“š
        json_data = fetch_epg_data(channel_id)
        if not json_data:
            failed_channels.append(channel_id)
            continue
            
        # æå–é »é“ä¿¡æ¯
        channel_info = get_channel_info(json_data, channel_id)
        if channel_info:
            all_channels_info.append(channel_info)
        
        # è§£æç¯€ç›®æ•¸æ“š
        programs = parse_epg_data(json_data, channel_id)
        all_programs.extend(programs)
            
        # éš¨æ©Ÿå»¶é²
        if idx < len(channels) - 1:
            delay = random.uniform(1, 3)
            print(f"â±ï¸ éš¨æ©Ÿå»¶é² {delay:.2f}ç§’")
            time.sleep(delay)
    
    # çµ±è¨ˆçµæœ
    print("\n" + "="*50)
    print(f"âœ… æˆåŠŸç²å– {len(all_channels_info)} å€‹é »é“ä¿¡æ¯")
    print(f"âœ… æˆåŠŸç²å– {len(all_programs)} å€‹ç¯€ç›®")
    
    if failed_channels:
        print(f"âš ï¸ å¤±æ•—é »é“ ({len(failed_channels)}): {', '.join(failed_channels)}")
    
    channel_counts = {}
    for program in all_programs:
        channel_counts[program["channelName"]] = channel_counts.get(program["channelName"], 0) + 1
    
    for channel, count in channel_counts.items():
        print(f"ğŸ“º é »é“ {channel}: {count} å€‹ç¯€ç›®")
    
    print("="*50)
    return all_channels_info, all_programs

def generate_xmltv(channels_info, programs, output_file="ofiii.xml"):
    """ç”ŸæˆXMLTVæ ¼å¼çš„EPGæ•¸æ“š"""
    print(f"\nç”ŸæˆXMLTVæª”æ¡ˆ: {output_file}")
    
    root = ET.Element("tv", generator="OFIII-EPG-Generator", source="www.ofiii.com")
    
    # æ·»åŠ é »é“å®šç¾©
    for channel in channels_info:
        channel_id = channel['id']
        channel_name = channel['channelName']
        
        channel_elem = ET.SubElement(root, "channel", id=channel_id)
        ET.SubElement(channel_elem, "display-name", lang="zh").text = channel_name
        
        if channel.get('logo'):
            ET.SubElement(channel_elem, "icon", src=channel['logo'])
    
    # æ·»åŠ ç¯€ç›®
    program_count = 0
    for program in programs:
        try:
            channel_id = program['channelName']
            start_time = program['start'].strftime('%Y%m%d%H%M%S %z')
            end_time = program['end'].strftime('%Y%m%d%H%M%S %z')
            
            program_elem = ET.SubElement(
                root, 
                "programme", 
                channel=channel_id,
                start=start_time, 
                stop=end_time
            )
            
            title = program.get('programName', 'æœªçŸ¥ç¯€ç›®')
            ET.SubElement(program_elem, "title", lang="zh").text = title
            
            if program.get('subtitle'):
                ET.SubElement(program_elem, "sub-title", lang="zh").text = program['subtitle']
            
            if program.get('description'):
                ET.SubElement(program_elem, "desc", lang="zh").text = program['description']
            
            program_count += 1
        except Exception as e:
            print(f"âš ï¸ è·³éç„¡æ•ˆçš„ç¯€ç›®æ•¸æ“š: {str(e)}")
            continue
    
    # ç”ŸæˆXML
    xml_str = ET.tostring(root, encoding='utf-8').decode('utf-8')
    
    try:
        parsed = minidom.parseString(xml_str)
        pretty_xml = parsed.toprettyxml(indent="  ", encoding='utf-8')
    except Exception as e:
        print(f"âš ï¸ XMLç¾åŒ–å¤±æ•—, ä½¿ç”¨åŸå§‹XML: {str(e)}")
        pretty_xml = xml_str.encode('utf-8')
    
    try:
        with open(output_file, 'wb') as f:
            f.write(pretty_xml)
        
        print(f"âœ… XMLTVæª”æ¡ˆå·²ç”Ÿæˆ: {output_file}")
        print(f"ğŸ“º é »é“æ•¸: {len(channels_info)}")
        print(f"ğŸ“º ç¯€ç›®æ•¸: {program_count}")
        print(f"ğŸ’¾ æª”æ¡ˆå¤§å°: {os.path.getsize(output_file) / 1024:.2f} KB")
        return True
    except Exception as e:
        print(f"âŒ å„²å­˜XMLæª”æ¡ˆå¤±æ•—: {str(e)}")
        return False

def generate_json_file(channels_info, output_file="ofiii.json"):
    """ç”ŸæˆJSONæ ¼å¼çš„é »é“æ•¸æ“š"""
    print(f"\nç”ŸæˆJSONæª”æ¡ˆ: {output_file}")
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(channels_info, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæª”æ¡ˆå·²ç”Ÿæˆ: {output_file}")
        print(f"ğŸ“º é »é“æ•¸: {len(channels_info)}")
        print(f"ğŸ’¾ æª”æ¡ˆå¤§å°: {os.path.getsize(output_file) / 1024:.2f} KB")
        
        # é¡¯ç¤ºå‰å¹¾å€‹é »é“ä½œç‚ºç¤ºä¾‹
        print("\nJSONæª”æ¡ˆå‰5å€‹é »é“ç¤ºä¾‹:")
        for i, channel in enumerate(channels_info[:5]):
            print(f"  {i+1}. {channel}")
            
        return True
    except Exception as e:
        print(f"âŒ å„²å­˜JSONæª”æ¡ˆå¤±æ•—: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•¸ï¼Œè™•ç†å‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='æ­é£›é›»è¦–ç¯€ç›®è¡¨')
    parser.add_argument('--output', type=str, default='output/ofiii.xml', 
                       help='è¼¸å‡ºXMLæª”æ¡ˆè·¯å¾‘ (é»˜èª: output/ofiii.xml)')
    
    args = parser.parse_args()
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"å»ºç«‹è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    try:
        # ç²å–EPGæ•¸æ“š
        channels_info, programs = get_ofiii_epg()
        
        if not channels_info:
            print("âŒ æœªç²å–åˆ°æœ‰æ•ˆé »é“ä¿¡æ¯ï¼Œç„¡æ³•ç”Ÿæˆæª”æ¡ˆ")
            sys.exit(1)
            
        # ç”ŸæˆXMLTVæª”æ¡ˆ
        xml_output = args.output
        if not generate_xmltv(channels_info, programs, xml_output):
            sys.exit(1)
            
        # ç”ŸæˆJSONæª”æ¡ˆ
        json_output = os.path.join(output_dir, "ofiii.json")
        if not generate_json_file(channels_info, json_output):
            print("âš ï¸ JSONæª”æ¡ˆç”Ÿæˆå¤±æ•—ï¼Œä½†XMLå·²æˆåŠŸç”Ÿæˆ")
            
    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
